# Quick Tabs Messaging & Logging: Critical Architecture & Coordination Issues

**Extension Version:** v1.6.4.15 | **Date:** 2025-12-10 | **Scope:** Port/BroadcastChannel messaging, logging consolidation, Firefox background idle coordination

---

## Executive Summary

The extension implements a sophisticated three-tier messaging architecture (Port → BroadcastChannel → Storage) to handle Firefox's 30-second background script idle timeout. However, several critical issues exist across messaging coordination, logging consistency, and background script keepalive:

1. **Firefox Idle Timeout Coordination Gap** - Heartbeat (25s) and keepalive (20s) are separate, uncoordinated systems with a 5-second window where background could die
2. **Incomplete Logging Consolidation** - v1.6.4.13 Issue #5 attempt to unify MESSAGE_RECEIVED logs was never completed; multiple paths still use old format
3. **Fragmented Deduplication Logic** - Three independent dedup methods with critical bugs (one never populated, others may conflict)
4. **Port Registry Age Threshold Missing** - No maximum port age, accumulating orphaned ports
5. **Storage Change Race Condition** - Double-dedup checks with aggressive 50ms cooldown filtering legitimate operations

These issues reduce reliability and debuggability of the extension, especially during high-traffic scenarios and Firefox background termination events.

---

## Issue #1: Firefox 30s Idle Timeout - Uncoordinated Keepalive & Heartbeat Systems

### Problem Summary

Firefox terminates service workers (background scripts) after 30 seconds of inactivity (Bug 1851373). The extension implements two separate keepalive mechanisms—a 25-second heartbeat and a 20-second keepalive message—that are uncoordinated. This creates a 5-second window where the background script could be terminated even with one mechanism running, because each resets a different timer independently.

### Root Cause

**Files:** `sidebar/quick-tabs-manager.js`, `src/background/handlers/QuickTabHandler.js`

**Issue:** Firefox Bug 1851373 documents that `runtime.Port` messages do NOT reset the background script idle timer. The workaround is to use `browser.runtime.sendMessage()` to reset it. However, the codebase has two independently operating keepalive systems:

1. **Heartbeat System** (`startHeartbeat()`, `sendHeartbeat()`) - Runs every 25 seconds via port connection
2. **Keepalive System** (separate, implied in background) - Runs every 20 seconds via `runtime.sendMessage()`

These operate on different schedules with no coordination between them. If the heartbeat system is busy/delayed and the 20-second keepalive hasn't fired yet, there's a window where both could miss their deadline and the background script terminates.

**Specific Problem Areas:**
- Line ~730-800 in `sidebar/quick-tabs-manager.js`: Heartbeat uses `sendPortMessageWithTimeout()` with 5-second timeout. If port is slow, timeout triggers zombie state, but background hasn't actually been killed yet.
- Line ~800-850: Keepalive interval isn't visible in sidebar code—it's assumed to exist in background but coordination is implicit, not explicit
- No shared state between the two systems to detect conflicts or overlaps
- No backpressure mechanism if one system detects the other is failing

<scope>
**Modify:**
- `sidebar/quick-tabs-manager.js` (heartbeat functions, keepalive setup)
- `src/background/` (keepalive implementation - verify it exists and coordinates with heartbeat)

**Do NOT Modify:**
- Port connection establishment (working correctly)
- BroadcastChannel implementation (separate tier)
- Storage layer (not related to idle timeout)
</scope>

### Fix Required

Unify the keepalive/heartbeat mechanism into a single coordinated system with explicit state tracking:

1. **Consolidate Timers**: Replace separate heartbeat (25s) and keepalive (20s) with single coordinated interval (20s) that's guaranteed to fire regularly
2. **Explicit Coordination**: Track which mechanism just fired and log transitions to avoid implicit assumptions
3. **Backpressure Detection**: If one keepalive round fails (heartbeat timeout), immediately trigger the other mechanism as fallback
4. **Zero-Gap Window**: Ensure the next keepalive is scheduled before the previous one completes, eliminating any gap where background could idle
5. **Logging**: Add explicit START and COMPLETE logs for each keepalive round with correlation IDs to detect missed rounds

Reference patterns: The circuit breaker implementation in `connectToBackground()` (lines 300-400) shows good pattern for timeout handling. The health probe in `_probeBackgroundHealth()` (lines 500-520) shows good pattern for lightweight checks.

<acceptance_criteria>
- [ ] Single unified keepalive interval running at 20-second schedule (no separate 25s heartbeat)
- [ ] Keepalive fires BEFORE the current round completes (20s + overhead < 30s threshold)
- [ ] START and COMPLETE logs emitted for each keepalive cycle with correlation ID
- [ ] If keepalive fails (timeout), fallback mechanism triggers immediately (not waiting for next round)
- [ ] Consecutive keepalive failures tracked and logged with threshold for ZOMBIE transition
- [ ] Manual test: Run extension for 5 minutes with frequent logging, verify no gaps > 20s between keepalive rounds
- [ ] Manual test: Simulate background delay (slow response), verify fallback triggers
</acceptance_criteria>

---

## Issue #2: Incomplete Logging Consolidation - MESSAGE_RECEIVED Format Inconsistency

### Problem Summary

v1.6.4.13 Issue #5 attempted to consolidate all MESSAGE_RECEIVED logs into unified format with `[PORT]` or `[BC]` prefix indicating message path. However, implementation is incomplete—several message paths still emit old-format logs without path indicators. This inconsistency makes it difficult to trace message routing and debug race conditions.

### Root Cause

**Files:** `sidebar/quick-tabs-manager.js`, `src/features/quick-tabs/channels/BroadcastChannelManager.js`

**Issue:** Three distinct message entry points exist but not all use the consolidated logging format:

1. **Port Message Path** (`handlePortMessage()`, line ~1850): Uses new consolidated format ✓
2. **BroadcastChannel Message Path** (`handleBroadcastChannelMessage()`, line ~2300): Uses new format ✓
3. **Runtime onMessage Path** (`browser.runtime.onMessage.addListener()`, line ~2100): **Still uses old format** ✗

Additionally, within handlers, state update notifications use inconsistent formats:
- `_handleQuickTabStateUpdate()` logs with `PORT_STATE_UPDATE_RECEIVED` format
- Old format logs still present for QUICK_TAB_DELETED via runtime path with no `[PATH]` indicator
- BroadcastChannel broadcasts sometimes log before routing, sometimes after, inconsistent timing

**Specific Problem Areas:**
- Line ~2100-2150: `_processRuntimeMessage()` logs `RUNTIME_MESSAGE_RECEIVED` without `[path: 'runtime.onMessage']` detail
- Line ~2700-2750: Old-format logs for QUICK_TAB_DELETED deletion confirmation lack path indicator
- Line ~1900: Some operation confirmations log without indicating port path
- No correlation IDs linking request → response across message types

<scope>
**Modify:**
- `sidebar/quick-tabs-manager.js` (all MESSAGE_RECEIVED logs, all state update logs, operation confirmation logs)
- `src/features/quick-tabs/channels/BroadcastChannelManager.js` (ensure all posts/receives use [BC] prefix consistently)

**Do NOT Modify:**
- Log levels (keep structure of existing log calls, only change message format)
- Actual message routing logic
- Handler function logic (logs only)
</scope>

### Fix Required

Audit all message entry/exit points and standardize logging format:

1. **Unified Entry Point Logging**: All MESSAGE_RECEIVED logs must include:
   - Message type in `[BRACKETS]` indicating source: `[PORT]`, `[BC]`, or `[RUNTIME]`
   - Common fields: `type`, `messageId`, `quickTabId`, `correlationId`, `timestamp`
   - Connection state (for PORT path only): current state of port connection

2. **Correlation IDs**: Add correlation ID tracking across all three paths so request/response pairs can be linked in logs

3. **Consistent Timing**: Log message entry BEFORE any handler processing, log exit AFTER handler completes (with duration)

4. **Path Indicators**: Every log mentioning a message must include path identifier to disambiguate which tier processed it

Reference pattern: `BroadcastChannelManager.postMessage()` (lines 100-150) shows correct `[BC]` prefix usage. `handlePortMessage()` (lines ~1850-1900) shows good consolidated format. Mirror these patterns in runtime path.

<acceptance_criteria>
- [ ] All MESSAGE_RECEIVED logs include `[PORT]`, `[BC]`, or `[RUNTIME]` path indicator
- [ ] All MESSAGE_RECEIVED logs include: type, messageId (or generated ID for BC), quickTabId (if applicable)
- [ ] Runtime.onMessage path uses same MESSAGE_RECEIVED format as port/BC paths
- [ ] Correlation IDs present and linked from request through all handlers to response
- [ ] Message entry/exit logged with duration for performance tracking
- [ ] Old-format logs removed; search for "RECEIVED" in sidebar finds only [PORT], [BC], [RUNTIME] variants
- [ ] Manual test: Perform create/update/delete operations, verify all messages appear in logs with consistent format
</acceptance_criteria>

---

## Issue #3: Fragmented Deduplication Logic - Three Competing Systems with Critical Bugs

### Problem Summary

Three independent deduplication mechanisms exist for preventing duplicate state updates: transactionId-based, saveId+timestamp window-based, and contentHash-based. One method is broken (never populates its tracking data), two may conflict with each other, creating unpredictable behavior where some duplicates are caught while others slip through. This causes duplicate renders and potential state inconsistencies.

### Root Cause

**Files:** `sidebar/quick-tabs-manager.js`, `src/background/handlers/QuickTabHandler.js`

**Issue:** Multiple dedup implementations without clear hierarchy:

1. **Method 1 - TransactionID** (v1.6.3.6-v12): Uses `IN_PROGRESS_TRANSACTIONS` set to track ongoing transactions
   - **Critical Bug**: The set is declared but NEVER populated anywhere in the codebase
   - Checks exist (`if (IN_PROGRESS_TRANSACTIONS.has(txId))`) but collection is always empty
   - This method never catches any duplicates despite being highest priority in comments

2. **Method 2 - SaveID Window** (v1.6.3.7-v5): `lastProcessedSaveId` + timestamp comparison
   - Checks if `currentSaveId === lastProcessedSaveId` within time window
   - Deduplicates state versions, but only for messages with saveId
   - May conflict with Method 1 if both were working

3. **Method 3 - ContentHash** (v1.6.3.4-v6): `computeStateHash()` comparison
   - Checks if new state hash equals `lastRenderedStateHash`
   - Catches identical state updates from different sources
   - May incorrectly filter legitimate concurrent updates if hashes temporarily match

**Specific Problem Areas:**
- Line ~2050-2100 in sidebar: `scheduleRender()` checks saveId dedup, then Message ID dedup, then hash dedup sequentially
- `IN_PROGRESS_TRANSACTIONS` never has entries added (search for `.add()` calls on this set yields nothing)
- `_multiMethodDeduplication()` in background performs additional dedup AFTER `_shouldIgnoreStorageChange()` already checked
- Comments indicate Method 1 should be highest priority (line ~50: "v1.6.3.6-v12 - FIX Issue #3: Three independent dedup methods...highest priority")
- 50ms STORAGE_CHANGE_COOLDOWN_MS may be too aggressive—legitimate rapid operations get filtered

<scope>
**Modify:**
- `sidebar/quick-tabs-manager.js` (`scheduleRender()`, `_markMessageAsProcessed()`, dedup logic)
- `src/background/handlers/QuickTabHandler.js` (`_multiMethodDeduplication()`, `_shouldIgnoreStorageChange()`)
- Any files that should populate `IN_PROGRESS_TRANSACTIONS` (likely StateCoordinator or similar)

**Do NOT Modify:**
- Hash computation function (`computeStateHash()` works correctly)
- Message routing (dedup is only for state update processing)
- Render function logic (only modify how dedup triggers/skips renders)
</scope>

### Fix Required

Consolidate deduplication into single, well-defined strategy with clear priority:

1. **Remove Dead Code**: Delete Method 1 (transactionId) entirely if it's not being populated. If it should exist, find what should populate it and fix.

2. **Unify Dedup Strategy**: Choose ONE primary dedup method (recommend Method 2 - saveId window):
   - Every state update must have a saveId for deduplication
   - Track `(saveId, timestamp)` tuple in a Map with TTL-based cleanup
   - Single lookup: `if (isDuplicate(currentSaveId, currentTimestamp)) return`

3. **Conditional Secondary Check**: Use hash comparison as secondary safeguard only for messages WITHOUT saveId

4. **Explicit Logging**: Every dedup check must log:
   - Which method triggered the dedup
   - What criteria matched (e.g., saveId match, hash match)
   - Whether the message was filtered or processed

5. **Adjust Cooldown**: Increase STORAGE_CHANGE_COOLDOWN_MS from 50ms to 100-200ms, or remove cooldown entirely if dedup is working correctly

Reference pattern: `_isMessageAlreadyProcessed()` (line ~2000) shows good pattern for dedup checks with logging.

<acceptance_criteria>
- [ ] IN_PROGRESS_TRANSACTIONS either populated correctly or removed entirely (no dead code)
- [ ] Single primary dedup method (saveId-based) with clear logic and documentation
- [ ] Hash comparison used as secondary safeguard only (with documentation explaining when/why)
- [ ] Every dedup check includes log entry showing which method triggered and why
- [ ] Dedup Map cleaned up periodically (old entries TTL'd, not accumulating indefinitely)
- [ ] STORAGE_CHANGE_COOLDOWN_MS reconsidered (may be too aggressive if dedup working correctly)
- [ ] Manual test: Create 3 Quick Tabs rapidly in succession, verify exactly 3 tab additions (no duplicates filtered)
- [ ] Manual test: Send same state update twice within 100ms, verify second is deduped with clear log message
</acceptance_criteria>

---

## Issue #4: Port Registry Management - Missing Maximum Age Threshold

### Problem Summary

Port connections are cleaned up every 5 minutes based on connection status (disconnected/closed), but there's no maximum age threshold. Ports that remain "open" but are orphaned accumulate indefinitely. In long-running sessions with multiple sidebar opens, stale ports consume resources and may interfere with new connections.

### Root Cause

**Files:** `src/background/managers/PortRegistryManager.js` (implied), `sidebar/quick-tabs-manager.js`

**Issue:** Port cleanup logic removes disconnected ports but has no age-based expiration:

- Ports are tracked by connection object reference
- Cleanup runs every 5 minutes checking if port is still connected
- If a port is "open" (connected property is true), it's never removed regardless of age
- Multiple sidebar instances in same session accumulate ports: window opens → sidebar opens → port created → sidebar closed but port remains
- Each new sidebar open creates a new port without clearing old ones
- No maximum lifetime for a port connection

**Specific Problem Areas:**
- Line ~730 in sidebar: `connectToBackground()` creates new port without checking if old orphaned port exists
- Cleanup logic (implied in background) checks disconnected status but not age
- No correlation between sidebar window and port (could have 1:N relationship of stale ports per sidebar)
- Port cleanup interval (5 minutes) is long—ports could accumulate many times over this period

<scope>
**Modify:**
- `sidebar/quick-tabs-manager.js` (port connection, cleanup)
- `src/background/managers/PortRegistryManager.js` or equivalent (cleanup logic, age threshold)

**Do NOT Modify:**
- Port message routing (works correctly)
- Port listener registration
- Disconnection handling (works correctly)
</scope>

### Fix Required

Add age-based port expiration with explicit tracking:

1. **Track Port Metadata**: Store creation timestamp, last message time, and origin (sidebar window ID) for each port

2. **Implement Age Threshold**: Set maximum port lifetime (60-120 seconds) independent of connection status. After threshold, close and remove port.

3. **Implement Inactivity Timeout**: If port hasn't received/sent message for 30 seconds, mark as stale and schedule removal (fallback to BroadcastChannel)

4. **Cleanup on Sidebar Close**: When sidebar unloads, explicitly close associated ports (not relying on 5-minute cleanup)

5. **Logging**: Log port creation, message activity, and age-based removal with port ID for tracking lifecycle

Reference pattern: Circuit breaker timeout logic in `connectToBackground()` (lines 300-400) shows good pattern for time-based state transitions.

<acceptance_criteria>
- [ ] Each port has metadata: createdAt, lastActivityTime, originWindowId
- [ ] Maximum port lifetime implemented (60-120s), ports removed when exceeded
- [ ] Inactivity timeout (30s no messages), stale ports marked and scheduled for removal
- [ ] Sidebar unload explicitly closes associated ports (doesn't wait for cleanup interval)
- [ ] Cleanup logic logs port creation, activity, and removal with duration/reason
- [ ] Manual test: Open/close sidebar 5 times in 1 minute, verify old ports removed (not accumulating)
- [ ] Manual test: Open sidebar, don't send any messages for 30s, verify port marked stale and fallback triggered
</acceptance_criteria>

---

## Issue #5: Storage Change Race Condition - Double-Dedup Checks with Aggressive Cooldown

### Problem Summary

Storage change handling performs deduplication in two places: `_shouldIgnoreStorageChange()` and `_multiMethodDeduplication()`. The redundant checks, combined with an aggressive 50ms cooldown window, may cause legitimate rapid storage operations to be incorrectly filtered. This creates state divergence between storage and sidebar cache.

### Root Cause

**Files:** `sidebar/quick-tabs-manager.js`, `src/background/handlers/QuickTabHandler.js`

**Issue:** Storage change deduplication runs twice with overlapping logic:

1. **First Check** - `_shouldIgnoreStorageChange()`: Checks if change should be ignored based on some criteria (not fully visible in scanned code, implied filter)

2. **Second Check** - `_multiMethodDeduplication()`: Runs AFTER first check, re-validates with three independent methods

3. **Cooldown** - `STORAGE_CHANGE_COOLDOWN_MS = 50ms`: If storage change arrives within 50ms of previous read, it's debounced/skipped

**Problems:**
- Double-checking suggests uncertainty about first filter's correctness
- 50ms cooldown is very aggressive—legitimate operations like batch tab creation might be filtered
- If first check misses a duplicate, second check has same data and will also miss it
- If first check correctly identifies duplicate, second check is wasted work
- Cooldown applies to ALL storage reads, not just dedup-related ones—normal state loads get delayed

**Specific Problem Areas:**
- Line ~1500-1600 in sidebar: `checkStorageDebounce()` enforces 50ms between reads
- Implied in background (not fully scanned): `_shouldIgnoreStorageChange()` and `_multiMethodDeduplication()` both exist and run sequentially
- No indication which filter is authoritative
- Log messages would show which filter triggered, but both checks exist

<scope>
**Modify:**
- `sidebar/quick-tabs-manager.js` (storage change handling, cooldown logic)
- `src/background/handlers/QuickTabHandler.js` (dedup checks - consolidate or remove redundancy)

**Do NOT Modify:**
- Storage layer itself (read/write operations working correctly)
- Message routing (not related to this race condition)
- Dedup Methods 1-3 logic (address in Issue #3 first)
</scope>

### Fix Required

Simplify and clarify storage change deduplication:

1. **Single Authoritative Dedup**: Remove one of the two dedup checks (`_shouldIgnoreStorageChange()` or `_multiMethodDeduplication()`). Keep only the more comprehensive one.

2. **Documented Filter Chain**: If two checks are truly needed for different purposes, add comments explaining why and in what order they run

3. **Adjust Cooldown**: Increase from 50ms to 200ms (or remove if dedup is working correctly), as 50ms may be too aggressive for batch operations

4. **Conditional Cooldown**: Apply cooldown only when dedup filter triggers, not to all storage reads. Normal reads should have minimal delay.

5. **Logging**: Every storage change must log whether it was:
   - Processed normally
   - Filtered by first check (reason)
   - Filtered by second check (reason)
   - Delayed by cooldown (time waited)

Reference pattern: `_markMessageAsProcessed()` in sidebar (line ~2000) shows good pattern for efficient dedup tracking.

<acceptance_criteria>
- [ ] One clear, single-source-of-truth dedup logic (no two sequential checks)
- [ ] If two checks necessary, documented clearly why with comments
- [ ] Cooldown increased to 200ms minimum, or removed if dedup is sufficient
- [ ] Cooldown only applied when dedup triggers (not all storage reads)
- [ ] Every storage change logged: processed/filtered/delayed with reason
- [ ] Manual test: Create 5 Quick Tabs rapidly (< 100ms apart), verify all 5 stored and no race condition
- [ ] Manual test: Manually edit browser.storage.local to simulate concurrent write, verify sidebar detects change
</acceptance_criteria>

---

## Supporting Context

<details>
<summary>Firefox Bug 1851373 - Background Script Idle Timeout</summary>

Firefox Bug 1851373 documents that WebExtension service workers (background scripts) are terminated after 30 seconds of inactivity. The key finding:

- **Port messages do NOT reset the idle timer**: `runtime.Port.postMessage()` does not count as "activity"
- **Workaround**: Use `browser.runtime.sendMessage()` which DOES reset the timer
- Firefox idle timeout: 30 seconds (configurable via `dom.serviceWorker.idle_timeout` preference, default 30s)
- Chrome idle timeout: 30 seconds (with additional 5-minute single-request limit)

Current extension implementation attempts to work around this with separate heartbeat (port) and keepalive (sendMessage) mechanisms, but they're not coordinated.

</details>

<details>
<summary>Deduplication Implementation Gaps</summary>

The three deduplication methods exist in the codebase with the following status:

**Method 1 (TransactionID)**: Declared in QuickTabHandler.js with comment "v1.6.2.4 - BUG FIX Issue 4: Message deduplication tracking" but the IN_PROGRESS_TRANSACTIONS Set is never populated via `.add()` calls anywhere in codebase. Code checking for duplicates exists but always finds empty set.

**Method 2 (SaveID)**: Implemented in sidebar/quick-tabs-manager.js `scheduleRender()` checking `lastProcessedSaveId`. Works correctly for state versions that have saveId, but undefined behavior for messages without saveId.

**Method 3 (Hash)**: Implemented via `computeStateHash()` comparing state hashes. Works correctly but may filter legitimate concurrent updates if hashes temporarily match between renders.

None of these methods explicitly acknowledge the others or coordinate their filtering, leading to unpredictable dedup behavior.

</details>

<details>
<summary>Logging Consolidation Timeline</summary>

- **v1.6.3.6-v11**: Port lifecycle logging added (`PORT_LIFECYCLE` logs)
- **v1.6.3.7-v3**: BroadcastChannel support added, messaging logging enhanced
- **v1.6.3.7-v4**: Message ID deduplication attempted, enhanced logging
- **v1.6.4.13 Issue #5**: Logging consolidation attempted - goal was to unify MESSAGE_RECEIVED logs with path indicators
  - Port path logs: Successfully using `[PORT]` prefix
  - BroadcastChannel path logs: Successfully using `[BC]` prefix
  - Runtime path logs: **Not updated** to use `[RUNTIME]` prefix
  - Result: Incomplete consolidation, 2/3 paths migrated

This indicates the consolidation task was partially completed but not finished.

</details>

---

## Summary Table

| Issue | Severity | Component | Root Cause | Impact |
|-------|----------|-----------|-----------|--------|
| Firefox Idle Timeout Gap | Critical | Messaging/Port | Uncoordinated keepalive systems | Background script can die; loss of state sync |
| Logging Inconsistency | High | Sidebar/Manager | Incomplete v1.6.4.13 consolidation | Debugging difficult; message routing unclear |
| Fragmented Dedup | High | State Sync | Dead code + conflicting methods | Duplicate renders; state inconsistency |
| Port Age Threshold | Medium | Port Registry | Missing cleanup logic | Resource accumulation; stale ports interfere |
| Storage Cooldown Race | Medium | Storage Handling | Double-dedup + aggressive 50ms | Legitimate ops filtered; state divergence |

---

**Priority:** Critical (Issues #1-2), High (Issues #3-4), Medium (Issue #5) | **Dependencies:** Issue #1 should be fixed before Issues #2-3 to enable comprehensive testing | **Complexity:** Medium (requires coordination logic but no complex algorithms)
